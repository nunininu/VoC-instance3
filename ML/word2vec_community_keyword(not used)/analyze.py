import os
from dotenv import load_dotenv
import psycopg2
import pandas as pd
from gensim.models import Word2Vec
from gensim import corpora, models
import networkx as nx

# .env ë¡œë“œ
load_dotenv()
conn = psycopg2.connect(
    host=os.getenv("DB_HOST"),
    port=os.getenv("DB_PORT"),
    dbname=os.getenv("DB_NAME"),
    user=os.getenv("DB_USER"),
    password=os.getenv("DB_PASSWORD")
)

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_sql("""
    SELECT analysis_result_id, client_id, consulting_id, keywords, positive, negative
    FROM analysis_result
    WHERE keywords IS NOT NULL AND positive IS NOT NULL AND negative IS NOT NULL
""", conn)

# ë¶„ë¥˜ ë¼ë²¨ ë¶€ì—¬
df["label"] = (df["positive"] > df["negative"]).astype(int)

# ë¶€ì • ì‘ë‹µì í•„í„°ë§ ë° í‚¤ì›Œë“œ ì „ì²˜ë¦¬
df_neg = df[df["label"] == 0].copy()
df_neg["tokenized"] = df_neg["keywords"].apply(lambda x: [w.strip() for w in x.split(",") if w.strip()])

# LDA ë¶„ì„
dictionary = corpora.Dictionary(df_neg["tokenized"])
corpus = [dictionary.doc2bow(text) for text in df_neg["tokenized"]]
lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)
print("ğŸ§  LDA Topics:")
for idx, topic in lda_model.print_topics(-1):
    print(f"Topic {idx}: {topic}")

# Word2Vec í•™ìŠµ
w2v_model = Word2Vec(sentences=df_neg["tokenized"], vector_size=100, window=3, min_count=1, workers=4)

# ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì„±
G = nx.Graph()
for kw in df_neg["tokenized"].explode().value_counts().head(5).index:
    if kw in w2v_model.wv:
        for sim_kw, score in w2v_model.wv.most_similar(kw, topn=5):
            G.add_edge(kw, sim_kw, weight=score)

# Supersetìš© í…Œì´ë¸”ë¡œ ì €ì¥
df["keyword"] = df["keywords"].str.split(",")
exploded = df.explode("keyword")
exploded["keyword"] = exploded["keyword"].str.strip()
exploded = exploded[exploded["keyword"] != ""]

from sqlalchemy import create_engine
engine = create_engine(f"postgresql+psycopg2://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}")
exploded.to_sql("analysis_keywords_exploded", engine, index=False, if_exists="replace")
