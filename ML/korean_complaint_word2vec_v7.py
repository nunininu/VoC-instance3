import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import networkx as nx
from pyvis.network import Network
from gensim.models import Word2Vec
from gensim.models.callbacks import CallbackAny2Vec
import re
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from sqlalchemy import create_engine
from dotenv import load_dotenv

# DB ì—°ê²°
load_dotenv()
DB_URL = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@" \
         f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
engine = create_engine(DB_URL)


# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'NanumSquareRound'
plt.rcParams['axes.unicode_minus'] = False

class Word2VecTrainingCallback(CallbackAny2Vec):
    """Word2Vec í•™ìŠµ ì§„í–‰ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±"""
    def __init__(self):
        self.epoch = 0
        self.losses = []

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        self.losses.append(loss)
        self.epoch += 1
        if self.epoch % 10 == 0:
            print(f'Epoch {self.epoch}: Loss = {loss}')

class WikiComplaintWord2VecSystem:
    def __init__(self):
        self.base_model = None  # ìœ„í‚¤í”¼ë””ì•„ ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸
        self.complaint_model = None  # ê³ ê°ë¶ˆë§Œ ì¶”ê°€í•™ìŠµ ëª¨ë¸
        self.vocabulary = None
        self.word_vectors = None
        load_dotenv()
        DB_URL = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@" \
                 f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
        self.engine = create_engine(DB_URL)

        
    def setup_mecab(self):
        """MeCab ì„¤ì¹˜ ë° ì„¤ì • (Colab í™˜ê²½ìš©)"""
        try:
            from mecab import MeCab
            self.mecab = MeCab()
            print("MeCabì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
        except:
            print("MeCab ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
            print("!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git")
            print("!cd Mecab-ko-for-Google-Colab && bash install_mecab-ko_on_colab190912.sh")
            return False
    
    def download_and_process_wikipedia(self, force_download=False):
        """ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("=== ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ===")
        
        import subprocess
        import os
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        wiki_dump_file = "kowiki-latest-pages-articles.xml.bz2"
        extracted_dir = "text"
        
        if not force_download and os.path.exists(extracted_dir):
            print(f"ì¶”ì¶œëœ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {extracted_dir}")
            return True
        
        try:
            # 1. wikiextractor ì„¤ì¹˜
            print("1. wikiextractor ì„¤ì¹˜ ì¤‘...")
            subprocess.run(["pip", "install", "wikiextractor"], check=False)
            
            # 2. ìœ„í‚¤í”¼ë””ì•„ ë¤í”„ ë‹¤ìš´ë¡œë“œ (íŒŒì¼ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
            if not os.path.exists(wiki_dump_file):
                print("2. ìœ„í‚¤í”¼ë””ì•„ ë¤í”„ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                download_cmd = [
                    "wget", 
                    "https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2"
                ]
                result = subprocess.run(download_cmd, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {result.stderr}")
                    print("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
                    print("wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2")
                    return False
            else:
                print(f"2. ìœ„í‚¤í”¼ë””ì•„ ë¤í”„ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {wiki_dump_file}")
            
            # 3. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if not os.path.exists(extracted_dir):
                print("3. ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                extract_cmd = [
                    "python", "-m", "wikiextractor.WikiExtractor", 
                    wiki_dump_file,
                    "--output", extracted_dir,
                    "--bytes", "100M",
                    "--processes", "4"
                ]
                result = subprocess.run(extract_cmd, capture_output=True, text=True)
                if result.returncode != 0:
                    print(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {result.stderr}")
                    return False
            else:
                print(f"3. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {extracted_dir}")
            
            print("ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("\nìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:")
            print("1. pip install wikiextractor")
            print("2. wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2")
            print("3. python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2")
            return False
    
    def list_wiki_files(self, dirname='text'):
        """ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ì˜ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        def list_wiki_recursive(dirname):
            filepaths = []
            if not os.path.exists(dirname):
                return filepaths
                
            filenames = os.listdir(dirname)
            for filename in filenames:
                filepath = os.path.join(dirname, filename)
                if os.path.isdir(filepath):
                    # ì¬ê·€ í•¨ìˆ˜
                    filepaths.extend(list_wiki_recursive(filepath))
                else:
                    find = re.findall(r"wiki_[0-9][0-9]", filepath)
                    if len(find) > 0:
                        filepaths.append(filepath)
            return sorted(filepaths)
        
        return list_wiki_recursive(dirname)
    
    def merge_wiki_files(self, filepaths, output_file="wiki_merged.txt"):
        """ìœ„í‚¤í”¼ë””ì•„ íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•©"""
        print(f"ì´ {len(filepaths)}ê°œì˜ íŒŒì¼ì„ í†µí•©í•©ë‹ˆë‹¤...")
        
        with open(output_file, "w", encoding="utf-8") as outfile:
            for filepath in tqdm(filepaths, desc="íŒŒì¼ í†µí•©"):
                try:
                    with open(filepath, encoding="utf-8") as infile:
                        contents = infile.read()
                        outfile.write(contents)
                except Exception as e:
                    print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {filepath}, {e}")
        
        print(f"í†µí•© ì™„ë£Œ: {output_file}")
        return output_file
    
    def preprocess_wiki_text(self, wiki_file="wiki_merged.txt", max_lines=100000):
        """ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í˜•íƒœì†Œ ë¶„ì„"""
        if not hasattr(self, 'mecab'):
            print("MeCabì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        print("ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        result = []
        line_count = 0
        
        try:
            with open(wiki_file, 'r', encoding='utf-8') as f:
                print(f"ìµœëŒ€ {max_lines}ê°œ ì¤„ê¹Œì§€ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
                
                # ì§„í–‰ ìƒí™© í‘œì‹œë¥¼ ìœ„í•œ ì¹´ìš´í„°
                processed_count = 0
                
                for line in f:
                    if line_count >= max_lines:
                        print(f"ìµœëŒ€ ì²˜ë¦¬ ì¤„ ìˆ˜({max_lines})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    line_count += 1
                    
                    # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆê³ , XML íƒœê·¸ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                    line = line.strip()
                    if line and not line.startswith('<') and not line.startswith('</'):
                        try:
                            morphs = self.mecab.morphs(line)
                            # ìµœì†Œ 3ê°œ ì´ìƒì˜ í˜•íƒœì†Œê°€ ìˆê³ , ê¸¸ì´ê°€ ì ì ˆí•œ ê²½ìš°ë§Œ
                            if len(morphs) >= 3 and len(morphs) <= 100:
                                # ë¶ˆìš©ì–´ì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
                                filtered_morphs = [
                                    word for word in morphs 
                                    if len(word) > 1 and word.isalnum()
                                ]
                                if len(filtered_morphs) >= 2:
                                    result.append(filtered_morphs)
                                    processed_count += 1
                        except Exception as e:
                            continue
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    if line_count % 10000 == 0:
                        print(f"ì²˜ë¦¬ëœ ì¤„: {line_count:,}, ìœ íš¨í•œ ë¬¸ì¥: {processed_count:,}")
                
        except Exception as e:
            print(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: ì´ {line_count:,}ì¤„ ì²˜ë¦¬, {len(result):,}ê°œì˜ ìœ íš¨í•œ ë¬¸ì¥ ìƒì„±")
        return result
    
    def train_base_wiki_model(self, processed_sentences, model_path="wiki_word2vec.model"):
        """ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ë¡œ ê¸°ë³¸ Word2Vec ëª¨ë¸ í•™ìŠµ"""
        print("=== ìœ„í‚¤í”¼ë””ì•„ Word2Vec ëª¨ë¸ í•™ìŠµ ì‹œì‘ ===")
        
        # Word2Vec ëª¨ë¸ ì„¤ì • (ì›ë³¸ ì˜ˆì‹œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°)
        self.base_model = Word2Vec(
            sentences=processed_sentences,
            vector_size=100,  # size -> vector_size (ìµœì‹  ë²„ì „)
            window=5,
            min_count=5,
            workers=4,
            sg=0  # CBOW ëª¨ë¸ ì‚¬ìš©
        )
        
        print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        print(f"ì–´íœ˜ í¬ê¸°: {len(self.base_model.wv.key_to_index)}")
        
        # ëª¨ë¸ ì €ì¥
        self.base_model.save(model_path)
        print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
        
        # í•™ìŠµ ê²°ê³¼ ì˜ˆì‹œ ì¶œë ¥
        self.test_base_model()
        
        return self.base_model
    
    def test_base_model(self):
        """ê¸°ë³¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print("\n=== ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
        
        test_words = ["ëŒ€í•œë¯¼êµ­", "í•œêµ­", "ì„œìš¸", "ë¶€ì‚°", "ì •ì¹˜", "ê²½ì œ", "ë¬¸í™”", "ê¸°ìˆ "]
        
        for word in test_words:
            if word in self.base_model.wv.key_to_index:
                try:
                    similar_words = self.base_model.wv.most_similar(word, topn=5)
                    print(f"\n'{word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:")
                    for sim_word, score in similar_words:
                        print(f"  {sim_word}: {score:.3f}")
                except:
                    print(f"'{word}' ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨")
    
    def load_base_model(self, model_path="wiki_word2vec.model"):
        """ì‚¬ì „ í•™ìŠµëœ ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸ ë¡œë“œ"""
        try:
            self.base_model = Word2Vec.load(model_path)
            print(f"ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            print(f"ì–´íœ˜ í¬ê¸°: {len(self.base_model.wv.key_to_index)}")
            return True
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def load_complaints_from_db(self, limit=10000):
        query = f"""
            SELECT c.content
            FROM consulting c
            JOIN analysis_result ar ON c.consulting_id = ar.consulting_id
            WHERE ar.is_negative = TRUE
            AND c.content IS NOT NULL
            AND LENGTH(c.content) > 5
            LIMIT {limit}
        """
        df = pd.read_sql(query, self.engine)
        return pd.DataFrame({'complaint': df['content'].tolist()})



    # def create_sample_complaint_data(self):
    #     """ê³ ê° ë¶ˆë§Œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    #     sample_complaints = [
    #         "ë°°ì†¡ì´ ë„ˆë¬´ ëŠ¦ì–´ì„œ ë¶ˆë§Œì…ë‹ˆë‹¤. ì•½ì†í•œ ë‚ ì§œë³´ë‹¤ ì¼ì£¼ì¼ì´ë‚˜ ì§€ì—°ë˜ì—ˆì–´ìš”.",
    #         "ìƒí’ˆ í’ˆì§ˆì´ ê¸°ëŒ€ì™€ ë‹¬ë¼ì„œ ì‹¤ë§í–ˆìŠµë‹ˆë‹¤. ì‚¬ì§„ê³¼ ì‹¤ì œ ì œí’ˆì´ ë„ˆë¬´ ë‹¬ë¼ìš”.",
    #         "ê³ ê°ì„œë¹„ìŠ¤ ì§ì›ì˜ ì‘ëŒ€ê°€ ë¶ˆì¹œì ˆí–ˆìŠµë‹ˆë‹¤. ë¬¸ì œ í•´ê²°ë„ ì œëŒ€ë¡œ ì•ˆ í•´ì£¼ë„¤ìš”.",
    #         "í™˜ë¶ˆ ì²˜ë¦¬ê°€ ë„ˆë¬´ ë³µì¡í•˜ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í•´ ì£¼ì„¸ìš”.",
    #         "ì›¹ì‚¬ì´íŠ¸ ì£¼ë¬¸ ì‹œìŠ¤í…œì— ì˜¤ë¥˜ê°€ ë§ì•„ì„œ ë¶ˆí¸í•©ë‹ˆë‹¤. ê²°ì œë„ ì œëŒ€ë¡œ ì•ˆ ë˜ê³ ìš”.",
    #         "ìƒí’ˆ í¬ì¥ì´ ì—‰ë§ì´ì–´ì„œ ì œí’ˆì´ ì†ìƒë˜ì—ˆìŠµë‹ˆë‹¤. í¬ì¥ ìƒíƒœë¥¼ ê°œì„ í•´ ì£¼ì„¸ìš”.",
    #         "ê°€ê²© ëŒ€ë¹„ í’ˆì§ˆì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ì´ ê°€ê²©ì´ë©´ ë” ì¢‹ì€ í’ˆì§ˆì„ ê¸°ëŒ€í–ˆì–´ìš”.",
    #         "AS ì„œë¹„ìŠ¤ê°€ ë¶€ì‹¤í•©ë‹ˆë‹¤. ìˆ˜ë¦¬ ê¸°ê°„ë„ ë„ˆë¬´ ê¸¸ê³  ë¹„ìš©ë„ ë¹„ì‹¸ìš”.",
    #         "ì§ì› êµìœ¡ì´ ë¶€ì¡±í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì œí’ˆì— ëŒ€í•œ ì •ë³´ë„ ì œëŒ€ë¡œ ëª¨ë¥´ë„¤ìš”.",
    #         "ë°°ì†¡ ì¶”ì  ì‹œìŠ¤í…œì´ ì •í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ë°°ì†¡ ìƒí™©ê³¼ ë‹¤ë¥´ê²Œ í‘œì‹œë˜ì–´ìš”.",
    #         "ìƒí’ˆ ì„¤ëª…ì´ ë¶€ì •í™•í•´ì„œ ì˜ëª» ì£¼ë¬¸í–ˆìŠµë‹ˆë‹¤. ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”.",
    #         "ë°˜í’ˆ ì •ì±…ì´ ë„ˆë¬´ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. ê³ ê° ì…ì¥ì„ ì¢€ ë” ê³ ë ¤í•´ ì£¼ì„¸ìš”.",
    #         "ì „í™” ìƒë‹´ ëŒ€ê¸°ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë¹ ë¥¸ ì‘ë‹µì„ ì›í•©ë‹ˆë‹¤.",
    #         "ì œí’ˆ í•˜ìê°€ ìˆëŠ”ë° êµí™˜ì´ ì–´ë µë‹¤ê³  í•˜ë„¤ìš”. í’ˆì§ˆ ê´€ë¦¬ ì¢€ í•´ì£¼ì„¸ìš”.",
    #         "ì˜¨ë¼ì¸ ì£¼ë¬¸ê³¼ ì‹¤ì œ ë°›ì€ ìƒí’ˆì´ ë‹¤ë¦…ë‹ˆë‹¤. ì£¼ë¬¸ ì‹œìŠ¤í…œì„ ì ê²€í•´ ì£¼ì„¸ìš”.",
    #         "ë§¤ì¥ ì§ì›ì´ ì œí’ˆ ì§€ì‹ì´ ë¶€ì¡±í•´ì„œ ì œëŒ€ë¡œ ì„¤ëª…ì„ ëª»í•´ì£¼ë„¤ìš”.",
    #         "ê²°ì œ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¤‘ë³µ ê²°ì œê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹ ë¥¸ í™˜ë¶ˆ ë°”ëë‹ˆë‹¤.",
    #         "ìƒí’ˆ ì¬ê³  ê´€ë¦¬ê°€ ì—‰ë§ì…ë‹ˆë‹¤. ì£¼ë¬¸ í›„ì— í’ˆì ˆì´ë¼ê³  í•˜ë„¤ìš”.",
    #         "ë°°ì†¡ë¹„ê°€ ë„ˆë¬´ ë¹„ìŒ‰ë‹ˆë‹¤. í•©ë¦¬ì ì¸ ë°°ì†¡ë¹„ ì •ì±…ì´ í•„ìš”í•´ìš”.",
    #         "ê³ ê°ì„¼í„° ìš´ì˜ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. 24ì‹œê°„ ì„œë¹„ìŠ¤ë¥¼ ì›í•©ë‹ˆë‹¤."
    #     ]
        
        # ë°ì´í„° í™•ì¥
        extended_complaints = []
        for complaint in sample_complaints:
            extended_complaints.append(complaint)
            # ë™ì¼í•œ ë¶ˆë§Œì„ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
            if "ë°°ì†¡" in complaint:
                extended_complaints.append("ë¬¼ê±´ì´ ì–¸ì œ ë„ì°©í•˜ëŠ”ì§€ ì•Œ ìˆ˜ê°€ ì—†ì–´ìš”. ë°°ì†¡ ì„œë¹„ìŠ¤ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            elif "í’ˆì§ˆ" in complaint:
                extended_complaints.append("ì œí’ˆ í€„ë¦¬í‹°ê°€ ë„ˆë¬´ ë–¨ì–´ì ¸ì„œ ëˆì´ ì•„ê¹ë„¤ìš”.")
            elif "ì„œë¹„ìŠ¤" in complaint:
                extended_complaints.append("ì§ì› ì„œë¹„ìŠ¤ êµìœ¡ì´ ì‹œê¸‰í•´ ë³´ì…ë‹ˆë‹¤. ë„ˆë¬´ ë¶ˆì¹œì ˆí•´ìš”.")
        
        return pd.DataFrame({'complaint': extended_complaints})
    
    def preprocess_complaint_text(self, text):
        """ê³ ê° ë¶ˆë§Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if pd.isna(text) or not hasattr(self, 'mecab'):
            return []
        
        try:
            # MeCab í˜•íƒœì†Œ ë¶„ì„
            morphs = self.mecab.morphs(str(text))
            
            # ë¶ˆìš©ì–´ ì œê±°
            stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 
                        'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ì—ê²Œ', 'í•œí…Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ì²˜ëŸ¼', 'ê°™ì´',
                        'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì•„ë‹ˆë‹¤', 'ê·¸ë¦¬ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤', 'í•´ìš”', 'ë„¤ìš”'}
            
            # ê¸¸ì´ í•„í„°ë§ ë° ë¶ˆìš©ì–´ ì œê±°
            words = [word for word in morphs if len(word) > 1 and word not in stopwords]
            return words
        except:
            return []
    
    def train_complaint_model(self, complaint_data, epochs=100):
        """ê³ ê° ë¶ˆë§Œ ë°ì´í„°ë¡œ ì¶”ê°€ í•™ìŠµ"""
        if self.base_model is None:
            print("ê¸°ë³¸ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”.")
            return None
        
        print("=== ê³ ê° ë¶ˆë§Œ ë°ì´í„° ì „ì²˜ë¦¬ ===")
        
        # ê³ ê° ë¶ˆë§Œ ë°ì´í„° ì „ì²˜ë¦¬
        processed_complaints = []
        for complaint in tqdm(complaint_data['complaint'], desc="ë¶ˆë§Œ ë°ì´í„° ì „ì²˜ë¦¬"):
            words = self.preprocess_complaint_text(complaint)
            if len(words) > 1:
                processed_complaints.append(words)
        
        print(f"ì „ì²˜ë¦¬ëœ ë¶ˆë§Œ ë¬¸ì¥ ìˆ˜: {len(processed_complaints)}")
        
        # ê¸°ì¡´ ëª¨ë¸ ë³µì‚¬í•˜ì—¬ ìƒˆ ëª¨ë¸ ìƒì„±
        print("=== ì¶”ê°€ í•™ìŠµ ì‹œì‘ ===")
        self.complaint_model = Word2Vec(
            vector_size=self.base_model.vector_size,
            window=self.base_model.window,
            min_count=1,  # ë¶ˆë§Œ ë°ì´í„°ëŠ” ì‘ìœ¼ë¯€ë¡œ min_countë¥¼ ë‚®ì¶¤
            workers=4,
            sg=self.base_model.sg
        )
        
        # ê¸°ë³¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìƒˆ ëª¨ë¸ì— ë³µì‚¬
        self.complaint_model.build_vocab(processed_complaints)
        
        # ê¸°ì¡´ ì–´íœ˜ì— ëŒ€í•œ ë²¡í„°ë¥¼ ë³µì‚¬
        for word in self.complaint_model.wv.key_to_index:
            if word in self.base_model.wv.key_to_index:
                self.complaint_model.wv.vectors[self.complaint_model.wv.key_to_index[word]] = \
                    self.base_model.wv.vectors[self.base_model.wv.key_to_index[word]]
        
        # ì¶”ê°€ í•™ìŠµ
        callback = Word2VecTrainingCallback()
        self.complaint_model.train(
            processed_complaints,
            total_examples=len(processed_complaints),
            epochs=epochs,
            callbacks=[callback]
        )
        
        print(f"ì¶”ê°€ í•™ìŠµ ì™„ë£Œ!")
        print(f"ìµœì¢… ì–´íœ˜ í¬ê¸°: {len(self.complaint_model.wv.key_to_index)}")
        
        # ì–´íœ˜ì™€ ë²¡í„° ì €ì¥
        self.vocabulary = list(self.complaint_model.wv.key_to_index.keys())
        self.word_vectors = np.array([self.complaint_model.wv[word] for word in self.vocabulary])
        
        return processed_complaints
    
    def compare_models(self):
        """ê¸°ë³¸ ëª¨ë¸ê³¼ ë¶ˆë§Œ ì¶”ê°€ í•™ìŠµ ëª¨ë¸ ë¹„êµ"""
        print("\n=== ëª¨ë¸ ë¹„êµ ë¶„ì„ ===")
        
        test_words = ["í’ˆì§ˆ", "ì„œë¹„ìŠ¤", "ë°°ì†¡", "ê°€ê²©", "ì§ì›", "ê³ ê°", "ë¬¸ì œ", "ë¶ˆë§Œ"]
        
        for word in test_words:
            print(f"\n--- '{word}' ìœ ì‚¬ ë‹¨ì–´ ë¹„êµ ---")
            
            # ê¸°ë³¸ ëª¨ë¸
            if word in self.base_model.wv.key_to_index:
                try:
                    base_similar = self.base_model.wv.most_similar(word, topn=5)
                    print("ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸:")
                    for sim_word, score in base_similar:
                        print(f"  {sim_word}: {score:.3f}")
                except:
                    print("ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸: ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨")
            else:
                print("ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸: ë‹¨ì–´ ì—†ìŒ")
            
            # ë¶ˆë§Œ ì¶”ê°€ í•™ìŠµ ëª¨ë¸
            if self.complaint_model and word in self.complaint_model.wv.key_to_index:
                try:
                    complaint_similar = self.complaint_model.wv.most_similar(word, topn=5)
                    print("ë¶ˆë§Œ ì¶”ê°€í•™ìŠµ ëª¨ë¸:")
                    for sim_word, score in complaint_similar:
                        print(f"  {sim_word}: {score:.3f}")
                except:
                    print("ë¶ˆë§Œ ì¶”ê°€í•™ìŠµ ëª¨ë¸: ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨")
            else:
                print("ë¶ˆë§Œ ì¶”ê°€í•™ìŠµ ëª¨ë¸: ë‹¨ì–´ ì—†ìŒ")
    
    def visualize_complaint_words(self, complaint_words=None, n_words=30):
        """ë¶ˆë§Œ ê´€ë ¨ ë‹¨ì–´ë“¤ì˜ ê´€ê³„ ì‹œê°í™”"""
        if self.complaint_model is None:
            print("ë¶ˆë§Œ í•™ìŠµ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if complaint_words is None:
            # ë¶ˆë§Œ ê´€ë ¨ ì£¼ìš” ë‹¨ì–´ë“¤
            complaint_words = ['ë¶ˆë§Œ', 'ë¬¸ì œ', 'ì„œë¹„ìŠ¤', 'í’ˆì§ˆ', 'ë°°ì†¡', 'ê°€ê²©', 'ì§ì›', 
                             'ê³ ê°', 'ì²˜ë¦¬', 'ê°œì„ ', 'ìš”ì²­', 'ì§€ì—°', 'ì˜¤ë¥˜', 'ì†ìƒ', 
                             'í™˜ë¶ˆ', 'êµí™˜', 'ìˆ˜ë¦¬', 'ë°˜í’ˆ', 'ìƒí’ˆ', 'ì œí’ˆ']
            
            # ì‹¤ì œ ëª¨ë¸ì— ìˆëŠ” ë‹¨ì–´ë“¤ë§Œ í•„í„°ë§
            complaint_words = [word for word in complaint_words 
                             if word in self.complaint_model.wv.key_to_index]
        
        if len(complaint_words) < 2:
            print("ì‹œê°í™”í•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # ì„ íƒëœ ë‹¨ì–´ë“¤ì˜ ë²¡í„° ì¶”ì¶œ
        word_vectors = np.array([self.complaint_model.wv[word] for word in complaint_words])
        
        # t-SNE ì ìš©
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, len(complaint_words)-1))
        word_vectors_2d = tsne.fit_transform(word_vectors)
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 10))
        
        # ë‹¨ì–´ íƒ€ì…ë³„ ìƒ‰ìƒ êµ¬ë¶„
        colors = []
        for word in complaint_words:
            if word in ['ë¶ˆë§Œ', 'ë¬¸ì œ', 'ì˜¤ë¥˜', 'ì†ìƒ']:
                colors.append('red')  # ë¶€ì •ì  ë‹¨ì–´
            elif word in ['ê°œì„ ', 'ì²˜ë¦¬', 'í•´ê²°', 'ìˆ˜ë¦¬']:
                colors.append('green')  # í•´ê²° ê´€ë ¨ ë‹¨ì–´
            elif word in ['ì„œë¹„ìŠ¤', 'í’ˆì§ˆ', 'ë°°ì†¡']:
                colors.append('blue')  # ì„œë¹„ìŠ¤ ê´€ë ¨ ë‹¨ì–´
            else:
                colors.append('gray')  # ê¸°íƒ€
        
        scatter = plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], 
                            c=colors, alpha=0.7, s=100)
        
        # ë‹¨ì–´ ë¼ë²¨ ì¶”ê°€
        for i, word in enumerate(complaint_words):
            plt.annotate(word, (word_vectors_2d[i, 0], word_vectors_2d[i, 1]),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=12, alpha=0.8, fontweight='bold')
        
        plt.title('ê³ ê° ë¶ˆë§Œ ë‹¨ì–´ ê´€ê³„ ì‹œê°í™” (ì¶”ê°€í•™ìŠµ ëª¨ë¸)', fontsize=16, fontweight='bold')
        plt.xlabel('t-SNE Dimension 1')
        plt.ylabel('t-SNE Dimension 2')
        plt.grid(True, alpha=0.3)
        
        # ë²”ë¡€ ì¶”ê°€
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='red', label='ë¶€ì •ì  ë‹¨ì–´'),
                          Patch(facecolor='green', label='í•´ê²° ê´€ë ¨'),
                          Patch(facecolor='blue', label='ì„œë¹„ìŠ¤ ê´€ë ¨'),
                          Patch(facecolor='gray', label='ê¸°íƒ€')]
        plt.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        import time
        import os

        # ì €ì¥ ê²½ë¡œ ì§€ì •
        save_path = "complaint_tsne_plot.png"

        # ê·¸ë¦¼ ì €ì¥
        plt.savefig(save_path)

        # ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì°½ ë„ìš°ë˜, 3ì´ˆ í›„ ìë™ ë‹«í˜
        plt.show(block=False)
        time.sleep(3)
        plt.close()

        # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ ì¶œë ¥
        print(f" TSNE ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {save_path}")
        print(f"    â€¢ ì „ì²´ ê²½ë¡œ: {os.path.abspath(save_path)}")
        
        # ë°˜í™˜ê°’ìœ¼ë¡œ 2D ë²¡í„°ì™€ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return word_vectors_2d, complaint_words
    
    def create_pyvis_network_graph(self, target_words=None, max_words=50, similarity_threshold=0.3, output_file="word_network.html"):
        """PyVisë¥¼ ì‚¬ìš©í•œ Force-directed ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±"""
        if self.complaint_model is None:
            print("ë¶ˆë§Œ í•™ìŠµ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print("=== PyVis ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ===")
        
        # íƒ€ê²Ÿ ë‹¨ì–´ ì„¤ì •
        if target_words is None:
            target_words = ['ë¶ˆë§Œ', 'ë¬¸ì œ', 'ì„œë¹„ìŠ¤', 'í’ˆì§ˆ', 'ë°°ì†¡', 'ê°€ê²©', 'ì§ì›', 
                           'ê³ ê°', 'ì²˜ë¦¬', 'ê°œì„ ', 'ìš”ì²­', 'ì§€ì—°', 'ì˜¤ë¥˜', 'ì†ìƒ', 
                           'í™˜ë¶ˆ', 'êµí™˜', 'ìˆ˜ë¦¬', 'ë°˜í’ˆ', 'ìƒí’ˆ', 'ì œí’ˆ', 'ë§Œì¡±',
                           'ì‹¤ë§', 'ë¶ˆí¸', 'í™”ë‚˜ë‹¤', 'ë‹µë‹µ', 'ë¹ ë¥´ë‹¤', 'ëŠë¦¬ë‹¤']
        
        # ëª¨ë¸ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ë“¤ë§Œ í•„í„°ë§
        available_words = [word for word in target_words if word in self.complaint_model.wv.key_to_index]
        
        if len(available_words) < 2:
            print("ì‹œê°í™”í•  ë‹¨ì–´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return None
        
        print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë‹¨ì–´ ìˆ˜: {len(available_words)}")
        
        # PyVis ë„¤íŠ¸ì›Œí¬ ê°ì²´ ìƒì„±
        net = Network(
            height="800px", 
            width="100%", 
            bgcolor="#222222", 
            font_color="white",
            directed=False
        )
        
        # ë¬¼ë¦¬í•™ ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • (Force-directed)
        net.set_options("""
        var options = {
            "physics": {
                "enabled": true,
                "forceAtlas2Based": {
                    "gravitationalConstant": -50,
                    "centralGravity": 0.01,
                    "springLength": 100,
                    "springConstant": 0.08,
                    "damping": 0.4,
                    "avoidOverlap": 0.5
                },
                "maxVelocity": 50,
                "minVelocity": 0.1,
                "solver": "forceAtlas2Based",
                "stabilization": {
                    "enabled": true,
                    "iterations": 500,
                    "updateInterval": 25
                }
            },
            "edges": {
                "smooth": {
                    "enabled": true,
                    "type": "continuous"
                }
            }
        }
        """)
        
        # ë‹¨ì–´ë³„ ì¹´í…Œê³ ë¦¬ ë° ìƒ‰ìƒ ì •ì˜
        word_categories = {
            'ë¶ˆë§Œ': {'category': 'negative', 'color': '#ff4444', 'size': 30},
            'ë¬¸ì œ': {'category': 'negative', 'color': '#ff4444', 'size': 25},
            'ì˜¤ë¥˜': {'category': 'negative', 'color': '#ff4444', 'size': 20},
            'ì†ìƒ': {'category': 'negative', 'color': '#ff4444', 'size': 20},
            'ì‹¤ë§': {'category': 'negative', 'color': '#ff6666', 'size': 20},
            'ë¶ˆí¸': {'category': 'negative', 'color': '#ff6666', 'size': 20},
            'í™”ë‚˜ë‹¤': {'category': 'negative', 'color': '#ff6666', 'size': 15},
            'ë‹µë‹µ': {'category': 'negative', 'color': '#ff6666', 'size': 15},
            'ê°œì„ ': {'category': 'solution', 'color': '#44ff44', 'size': 25},
            'ì²˜ë¦¬': {'category': 'solution', 'color': '#44ff44', 'size': 20},
            'í•´ê²°': {'category': 'solution', 'color': '#44ff44', 'size': 20},
            'ìˆ˜ë¦¬': {'category': 'solution', 'color': '#66ff66', 'size': 15},
            'ì„œë¹„ìŠ¤': {'category': 'service', 'color': '#4444ff', 'size': 25},
            'í’ˆì§ˆ': {'category': 'service', 'color': '#4444ff', 'size': 25},
            'ë°°ì†¡': {'category': 'service', 'color': '#6666ff', 'size': 20},
            'ì§ì›': {'category': 'service', 'color': '#6666ff', 'size': 20},
            'ê³ ê°': {'category': 'neutral', 'color': '#ffff44', 'size': 25},
            'ìƒí’ˆ': {'category': 'neutral', 'color': '#ffaa44', 'size': 20},
            'ì œí’ˆ': {'category': 'neutral', 'color': '#ffaa44', 'size': 20},
            'ê°€ê²©': {'category': 'neutral', 'color': '#ffaa44', 'size': 20},
            'ë§Œì¡±': {'category': 'positive', 'color': '#44ffaa', 'size': 20},
            'ë¹ ë¥´ë‹¤': {'category': 'positive', 'color': '#44ffaa', 'size': 15},
            'ëŠë¦¬ë‹¤': {'category': 'negative', 'color': '#ff8888', 'size': 15}
        }
        
        # ë…¸ë“œ ì¶”ê°€
        node_positions = {}
        for word in available_words:
            word_info = word_categories.get(word, {'category': 'other', 'color': '#cccccc', 'size': 15})
            
            net.add_node(
                word, 
                label=word,
                color=word_info['color'],
                size=word_info['size'],
                title=f"ì¹´í…Œê³ ë¦¬: {word_info['category']}<br>ë‹¨ì–´: {word}",
                font={'size': 14, 'color': 'white'}
            )
        
        # ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ë° ê°„ì„  ì¶”ê°€
        print("ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        edge_count = 0
        
        for i, word1 in enumerate(available_words):
            for j, word2 in enumerate(available_words[i+1:], i+1):
                try:
                    # ë‘ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = self.complaint_model.wv.similarity(word1, word2)
                    
                    # ì„ê³„ê°’ë³´ë‹¤ ë†’ì€ ìœ ì‚¬ë„ë§Œ ê°„ì„ ìœ¼ë¡œ ì¶”ê°€
                    if similarity > similarity_threshold:
                        # ìœ ì‚¬ë„ì— ë”°ë¥¸ ê°„ì„  êµµê¸° ë° ìƒ‰ìƒ ì„¤ì •
                        edge_width = max(1, similarity * 10)  # 1~10 ì‚¬ì´ì˜ êµµê¸°
                        
                        if similarity > 0.7:
                            edge_color = '#ff4444'  # ë†’ì€ ìœ ì‚¬ë„ - ë¹¨ê°„ìƒ‰
                        elif similarity > 0.5:
                            edge_color = '#ffaa44'  # ì¤‘ê°„ ìœ ì‚¬ë„ - ì£¼í™©ìƒ‰
                        else:
                            edge_color = '#4444ff'  # ë‚®ì€ ìœ ì‚¬ë„ - íŒŒë€ìƒ‰
                        
                        net.add_edge(
                            word1, 
                            word2, 
                            width=edge_width,
                            color=edge_color,
                            title=f"{word1} â†” {word2}<br>ìœ ì‚¬ë„: {similarity:.3f}",
                            length=max(50, (1-similarity) * 200)  # ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ì§§ì€ ê°„ì„ 
                        )
                        edge_count += 1
                        
                except Exception as e:
                    continue
        
        print(f"ìƒì„±ëœ ê°„ì„  ìˆ˜: {edge_count}")
        
        # HTML íŒŒì¼ë¡œ ì €ì¥
        try:
            net.save_graph(output_file)
            print(f"ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {output_file}")
            
            # â–¶ CSV íŒŒì¼ ì €ì¥
            nodes_df = pd.DataFrame([{
                'id': node['id'],
                'label': node['label'],
                'category': node.get('title', '').split('<br>')[0].replace("ì¹´í…Œê³ ë¦¬: ", ''),
                'size': node.get('size', 15)
            } for node in net.nodes])
            
            edges_df = pd.DataFrame([{
                'source': edge['from'],
                'target': edge['to'],
                'weight': edge.get('width', 1),
                'similarity': float(edge['title'].split("ìœ ì‚¬ë„: ")[-1]) if "ìœ ì‚¬ë„" in edge['title'] else None
            } for edge in net.edges])
            
            nodes_df.to_csv("complaint_nodes_clean.csv", index=False)
            edges_df.to_csv("complaint_edges_clean.csv", index=False)
            
            print("ğŸ“ complaint_nodes_clean.csv / complaint_edges_clean.csv íŒŒì¼ ì €ì¥ ì™„ë£Œ")
            
            # ì›¹ë¸Œë¼ìš°ì €ì—ì„œ ìë™ ì—´ê¸°
            import webbrowser
            import os
            file_path = os.path.abspath(output_file)
            webbrowser.open(f'file://{file_path}')
            print(f"ì›¹ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°: file://{file_path}")
            
            return output_file
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def create_interactive_similarity_network(self, focus_word="ë¶ˆë§Œ", top_n=15, output_file="similarity_network.html"):
        """íŠ¹ì • ë‹¨ì–´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ìƒì„±"""
        if self.complaint_model is None:
            print("ë¶ˆë§Œ í•™ìŠµ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if focus_word not in self.complaint_model.wv.key_to_index:
            print(f"ë‹¨ì–´ '{focus_word}'ê°€ ëª¨ë¸ì— ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"=== '{focus_word}' ì¤‘ì‹¬ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ìƒì„± ===")
        
        # ì¤‘ì‹¬ ë‹¨ì–´ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ ì°¾ê¸°
        try:
            similar_words = self.complaint_model.wv.most_similar(focus_word, topn=top_n)
            all_words = [focus_word] + [word for word, _ in similar_words]
            
        except Exception as e:
            print(f"ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            return None
        
        # PyVis ë„¤íŠ¸ì›Œí¬ ìƒì„±
        net = Network(
            height="700px", 
            width="100%", 
            bgcolor="#111111", 
            font_color="white",
            directed=False
        )
        
        # ë¬¼ë¦¬í•™ ì„¤ì •
        net.set_options("""
        var options = {
            "physics": {
                "enabled": true,
                "barnesHut": {
                    "gravitationalConstant": -30000,
                    "centralGravity": 1,
                    "springLength": 150,
                    "springConstant": 0.05,
                    "damping": 0.09,
                    "avoidOverlap": 0.5
                },
                "maxVelocity": 50,
                "solver": "barnesHut",
                "stabilization": {
                    "enabled": true,
                    "iterations": 200
                }
            }
        }
        """)
        
        # ì¤‘ì‹¬ ë…¸ë“œ ì¶”ê°€
        net.add_node(
            focus_word,
            label=focus_word,
            color='#ff4444',
            size=40,
            title=f"ì¤‘ì‹¬ ë‹¨ì–´: {focus_word}",
            font={'size': 18, 'color': 'white'}
        )
        
        # ìœ ì‚¬ ë‹¨ì–´ ë…¸ë“œ ë° ê°„ì„  ì¶”ê°€
        for word, similarity in similar_words:
            # ìœ ì‚¬ë„ì— ë”°ë¥¸ ë…¸ë“œ í¬ê¸° ë° ìƒ‰ìƒ
            node_size = 15 + similarity * 20  # 15~35 ì‚¬ì´
            
            if similarity > 0.7:
                node_color = '#ff6666'
            elif similarity > 0.5:
                node_color = '#ffaa66'
            elif similarity > 0.3:
                node_color = '#66aaff'
            else:
                node_color = '#aaaaaa'
            
            # ë…¸ë“œ ì¶”ê°€
            net.add_node(
                word,
                label=word,
                color=node_color,
                size=node_size,
                title=f"ë‹¨ì–´: {word}<br>ìœ ì‚¬ë„: {similarity:.3f}",
                font={'size': 12, 'color': 'white'}
            )
            
            # ì¤‘ì‹¬ ë‹¨ì–´ì™€ ê°„ì„  ì—°ê²°
            edge_width = max(2, similarity * 8)
            net.add_edge(
                focus_word,
                word,
                width=edge_width,
                color='#ffffff',
                title=f"ìœ ì‚¬ë„: {similarity:.3f}",
                length=max(100, (1-similarity) * 300)
            )
        
        # ìœ ì‚¬ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë„ ì¶”ê°€ (ì˜µì…˜)
        for i, (word1, sim1) in enumerate(similar_words[:10]):  # ìƒìœ„ 10ê°œë§Œ
            for j, (word2, sim2) in enumerate(similar_words[i+1:10], i+1):
                try:
                    cross_similarity = self.complaint_model.wv.similarity(word1, word2)
                    if cross_similarity > 0.4:  # ë†’ì€ ìœ ì‚¬ë„ë§Œ
                        net.add_edge(
                            word1,
                            word2,
                            width=cross_similarity * 3,
                            color='#666666',
                            title=f"{word1} â†” {word2}<br>ìœ ì‚¬ë„: {cross_similarity:.3f}",
                            length=200
                        )
                except:
                    continue
        
        # ì €ì¥ ë° ì—´ê¸°
        try:
            net.save_graph(output_file)
            print(f"ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ì €ì¥ ì™„ë£Œ: {output_file}")
            
            import webbrowser
            import os
            file_path = os.path.abspath(output_file)
            webbrowser.open(f'file://{file_path}')
            
            return output_file
            
        except Exception as e:
            print(f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
    
    def analyze_complaint_sentiment_words(self):
        """ë¶ˆë§Œ ê°ì • ë‹¨ì–´ ë¶„ì„"""
        if self.complaint_model is None:
            print("ë¶ˆë§Œ í•™ìŠµ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("=== ë¶ˆë§Œ ê°ì • ë‹¨ì–´ ë¶„ì„ ===")
        
        # ê°ì • í‚¤ì›Œë“œë³„ ìœ ì‚¬ ë‹¨ì–´ ë¶„ì„
        emotion_keywords = {
            'ë¶ˆë§Œ': 'ë¶€ì •ì  ê°ì •',
            'ì‹¤ë§': 'ì‹¤ë§ê°',
            'ë¶ˆí¸': 'ë¶ˆí¸í•¨',
            'í™”': 'ë¶„ë…¸',
            'ë‹µë‹µ': 'ë‹µë‹µí•¨',
            'ì§œì¦': 'ì§œì¦',
            'ê°œì„ ': 'ê°œì„  ìš”êµ¬',
            'ìš”ì²­': 'ìš”ì²­ì‚¬í•­'
        }
        
        for keyword, category in emotion_keywords.items():
            if keyword in self.complaint_model.wv.key_to_index:
                try:
                    similar_words = self.complaint_model.wv.most_similar(keyword, topn=10)
                    print(f"\n[{category}] '{keyword}' ê´€ë ¨ ë‹¨ì–´ë“¤:")
                    for word, score in similar_words:
                        print(f"  {word}: {score:.3f}")
                except:
                    print(f"'{keyword}' ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨")
    
    def save_models(self, base_path="wiki_word2vec.model", complaint_path="complaint_word2vec.model"):
        """ëª¨ë¸ë“¤ ì €ì¥"""
        if self.base_model:
            self.base_model.save(base_path)
            print(f"ê¸°ë³¸ ëª¨ë¸ ì €ì¥: {base_path}")
        
        if self.complaint_model:
            self.complaint_model.save(complaint_path)
            print(f"ë¶ˆë§Œ í•™ìŠµ ëª¨ë¸ ì €ì¥: {complaint_path}")
    
    def quick_demo(self):
        """ë¹ ë¥¸ ë°ëª¨ë¥¼ ìœ„í•œ í•¨ìˆ˜"""
        print("=== ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰ ===")
        
        # ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹œì‘
        if self.base_model is None:
            print("ê¸°ë³¸ ìƒ˜í”Œ ëª¨ë¸ ìƒì„± ì¤‘...")
            base_sentences = [
                ['ëŒ€í•œë¯¼êµ­', 'í•œêµ­', 'ì„œìš¸', 'ë¶€ì‚°'],
                ['í’ˆì§ˆ', 'ì„œë¹„ìŠ¤', 'ê³ ê°', 'ë§Œì¡±', 'ë¶ˆë§Œ'],
                ['ë°°ì†¡', 'ì£¼ë¬¸', 'ê²°ì œ', 'í™˜ë¶ˆ'],
                ['ì§ì›', 'ì‘ë‹µ', 'ì²˜ë¦¬', 'ì‹œê°„'],
                ['ìƒí’ˆ', 'ì œí’ˆ', 'ê°€ê²©', 'ë¹„ìš©'],
                ['ë¬¸ì œ', 'ì˜¤ë¥˜', 'í•´ê²°', 'ë„ì›€']
            ]
            
            self.base_model = Word2Vec(
                sentences=base_sentences,
                vector_size=50,  # ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶•ì†Œ
                window=3,
                min_count=1,
                workers=2,
                sg=0
            )
        
        # ê°„ë‹¨í•œ ë¶ˆë§Œ ë°ì´í„° ìƒì„±
        simple_complaints = [
            "ë°°ì†¡ì´ ëŠ¦ì–´ì„œ ë¶ˆë§Œì…ë‹ˆë‹¤",
            "ìƒí’ˆ í’ˆì§ˆì´ ë‚˜ì©ë‹ˆë‹¤",
            "ì„œë¹„ìŠ¤ê°€ ë¶ˆì¹œì ˆí•©ë‹ˆë‹¤",
            "ê°€ê²©ì´ ë„ˆë¬´ ë¹„ìŒ‰ë‹ˆë‹¤",
            "ë¬¸ì œê°€ í•´ê²°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        ]
        
        # simple_complaints = [c for c in simple_complaints if 'ë°°ì†¡' not in c]

        
        complaint_df = pd.DataFrame({'complaint': simple_complaints})
        complaint_df = complaint_df[~complaint_df['complaint'].str.contains('ë°°ì†¡', na=False)]

        
        # ë¹ ë¥¸ í•™ìŠµ (10 ì—í¬í¬)
        self.train_complaint_model(complaint_df, epochs=10)
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        if self.complaint_model:
            test_words = ['ë°°ì†¡', 'í’ˆì§ˆ', 'ì„œë¹„ìŠ¤']
            for word in test_words:
                if word in self.complaint_model.wv.key_to_index:
                    similar = self.complaint_model.wv.most_similar(word, topn=2)
                    print(f"{word}: {similar}")
            
            # ê°„ë‹¨í•œ PyVis ë„¤íŠ¸ì›Œí¬ ìƒì„±
            try:
                print("\nê°„ë‹¨í•œ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
                self.create_interactive_similarity_network(
                    focus_word="ë¶ˆë§Œ",
                    top_n=8,
                    output_file="demo_network.html"
                )
            except Exception as e:
                print(f"ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
        
        print("ë¹ ë¥¸ ë°ëª¨ ì™„ë£Œ!")

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main_wiki_complaint_system(use_wiki_data=True, force_retrain=False):
    """ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    print("=== ìœ„í‚¤í”¼ë””ì•„ + ê³ ê°ë¶ˆë§Œ Word2Vec ì‹œìŠ¤í…œ ===")
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    system = WikiComplaintWord2VecSystem()
    
    # MeCab ì„¤ì • í™•ì¸
    if not system.setup_mecab():
        print("MeCab ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        return None
    
    # ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
    model_exists = system.load_base_model("wiki_word2vec.model")
    
    if not model_exists or force_retrain:
        if not model_exists:
            print("\nì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("\nê°•ì œ ì¬í•™ìŠµ ëª¨ë“œì…ë‹ˆë‹¤.")
            
        if use_wiki_data:
            print("ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„°ë¡œ ìƒˆ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤...")
            
            # ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬
            wiki_success = system.download_and_process_wikipedia()
            
            if wiki_success:
                # ìœ„í‚¤í”¼ë””ì•„ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                wiki_files = system.list_wiki_files('text')
                print(f"ë°œê²¬ëœ ìœ„í‚¤í”¼ë””ì•„ íŒŒì¼ ìˆ˜: {len(wiki_files)}")
                
                if len(wiki_files) > 0:
                    # íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì¼ë¶€ë§Œ ì²˜ë¦¬)
                    print("ìœ„í‚¤í”¼ë””ì•„ íŒŒì¼ í†µí•© ì¤‘...")
                    # ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    limited_files = wiki_files[:min(10, len(wiki_files))]
                    merged_file = system.merge_wiki_files(limited_files)
                    
                    # ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                    print("ìœ„í‚¤í”¼ë””ì•„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° í˜•íƒœì†Œ ë¶„ì„ ì¤‘...")
                    processed_sentences = system.preprocess_wiki_text(merged_file)
                    
                    if len(processed_sentences) > 0:
                        # ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸ í•™ìŠµ
                        print("ìœ„í‚¤í”¼ë””ì•„ Word2Vec ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
                        system.train_base_wiki_model(processed_sentences)
                    else:
                        print("ì „ì²˜ë¦¬ëœ ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                        use_wiki_data = False
                else:
                    print("ìœ„í‚¤í”¼ë””ì•„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    use_wiki_data = False
            else:
                print("ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                use_wiki_data = False
        
        # ìœ„í‚¤í”¼ë””ì•„ í•™ìŠµì— ì‹¤íŒ¨í–ˆê±°ë‚˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ìƒì„±
        if not use_wiki_data or system.base_model is None:
            print("ê¸°ë³¸ ìƒ˜í”Œ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            
            # ê¸°ë³¸ í•œêµ­ì–´ ë¬¸ì¥ë“¤ë¡œ ì´ˆê¸° ëª¨ë¸ ìƒì„±
            base_sentences = [
                ['ëŒ€í•œë¯¼êµ­', 'í•œêµ­', 'ì„œìš¸', 'ë¶€ì‚°', 'ì •ì¹˜', 'ê²½ì œ'],
                ['í’ˆì§ˆ', 'ì„œë¹„ìŠ¤', 'ê³ ê°', 'ë§Œì¡±', 'ë¶ˆë§Œ', 'ê°œì„ '],
                ['ë°°ì†¡', 'ì£¼ë¬¸', 'ê²°ì œ', 'í™˜ë¶ˆ', 'êµí™˜', 'ë°˜í’ˆ'],
                ['ì§ì›', 'ì‘ë‹µ', 'ì²˜ë¦¬', 'ì‹œê°„', 'ë¹ ë¥¸', 'ëŠë¦°'],
                ['ìƒí’ˆ', 'ì œí’ˆ', 'ê°€ê²©', 'ë¹„ìš©', 'ì €ë ´', 'ë¹„ì‹¼'],
                ['ë¬¸ì œ', 'ì˜¤ë¥˜', 'í•´ê²°', 'ë„ì›€', 'ì§€ì›', 'ìƒë‹´'],
                ['ì›¹ì‚¬ì´íŠ¸', 'ì‹œìŠ¤í…œ', 'ì˜¨ë¼ì¸', 'ì˜¤í”„ë¼ì¸', 'ë§¤ì¥'],
                ['í¬ì¥', 'ë°°ì†¡', 'íƒë°°', 'ìš´ì†¡', 'ë„ì°©', 'ì§€ì—°']
            ]
            
            system.base_model = Word2Vec(
                sentences=base_sentences,
                vector_size=100,
                window=5,
                min_count=1,
                workers=4,
                sg=0
            )
            print("ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # ê¸°ë³¸ ëª¨ë¸ë„ ì €ì¥
            system.base_model.save("wiki_word2vec.model")
            print("ê¸°ë³¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
    else:
        print("ê¸°ì¡´ ìœ„í‚¤í”¼ë””ì•„ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ê³ ê° ë¶ˆë§Œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    # print("\n=== ê³ ê° ë¶ˆë§Œ ë°ì´í„° ìƒì„± ===")
    # complaint_data = system.create_sample_complaint_data()
    # print(f"ìƒì„±ëœ ë¶ˆë§Œ ë°ì´í„° ìˆ˜: {len(complaint_data)}")
    
    # ğŸ” ê³ ê° ë¶ˆë§Œ ë°ì´í„° ë¡œë”©: DBì—ì„œ ê°€ì ¸ì˜´
    print("\n=== DBì—ì„œ ê³ ê° ë¶ˆë§Œ ë°ì´í„° ë¡œë“œ ===")
    complaint_data = system.load_complaints_from_db(limit=10000)
    complaint_data = complaint_data[~complaint_data['complaint'].str.contains("ë°°ì†¡")]
    print(f"ğŸš« 'ë°°ì†¡' ì œì™¸ í›„ ë¶ˆë§Œ ë°ì´í„° ìˆ˜: {len(complaint_data)}")
    print(f"â–¶ ë¶ˆëŸ¬ì˜¨ ë¶ˆë§Œ ë°ì´í„° ìˆ˜: {len(complaint_data)}")
    
    # ğŸ” Word2Vec ì¶”ê°€ í•™ìŠµ
    print("\n=== ê³ ê° ë¶ˆë§Œ ì¶”ê°€ í•™ìŠµ ===")
    processed_complaints = system.train_complaint_model(complaint_data, epochs=50)
    
    # ëª¨ë¸ ë¹„êµ ë¶„ì„
    system.compare_models()
    
    # ë¶ˆë§Œ ë‹¨ì–´ ê´€ê³„ ì‹œê°í™”
    print("\n=== ë¶ˆë§Œ ë‹¨ì–´ ê´€ê³„ ì‹œê°í™” ===")
    system.visualize_complaint_words()
    
    # PyVis ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™”
    print("\n=== PyVis ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ì‹œê°í™” ===")
    try:
        # ì „ì²´ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„
        system.create_pyvis_network_graph(
            similarity_threshold=0.2,
            output_file="complaint_word_network.html"
        )
        
        # íŠ¹ì • ë‹¨ì–´ ì¤‘ì‹¬ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬
        system.create_interactive_similarity_network(
            focus_word="ë¶ˆë§Œ",
            top_n=20,
            output_file="complaint_similarity_network.html"
        )
        
    except ImportError:
        print("PyVis ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install pyvis")
    except Exception as e:
        print(f"PyVis ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ë¶ˆë§Œ ê°ì • ë‹¨ì–´ ë¶„ì„
    system.analyze_complaint_sentiment_words()
    
    # ëª¨ë¸ ì €ì¥
    print("\n=== ëª¨ë¸ ì €ì¥ ===")
    system.save_models()
    
    return system

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    import argparse
    
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description='Korean Complaint Word2Vec System')
    parser.add_argument('--no-wiki', action='store_true', 
                       help='ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ì‚¬ìš©í•˜ì§€ ì•Šê³  ê¸°ë³¸ ëª¨ë¸ë§Œ ì‚¬ìš©')
    parser.add_argument('--force-retrain', action='store_true', 
                       help='ê¸°ì¡´ ëª¨ë¸ì´ ìˆì–´ë„ ê°•ì œë¡œ ì¬í•™ìŠµ')
    parser.add_argument('--sample-only', action='store_true',
                       help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê¸°ë³¸ ìƒ˜í”Œ ëª¨ë¸ë§Œ ì‚¬ìš©')
    
    args = parser.parse_args()
    
    if args.sample_only:
        print("=== ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ê¸°ë³¸ ìƒ˜í”Œ ëª¨ë¸ë§Œ ì‚¬ìš©) ===")
        system = main_wiki_complaint_system(use_wiki_data=False, force_retrain=False)
    else:
        # ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰
        use_wiki = not args.no_wiki
        force_retrain = args.force_retrain
        
        print(f"=== ì‹œìŠ¤í…œ ì„¤ì • ===")
        print(f"ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ì‚¬ìš©: {'ì˜ˆ' if use_wiki else 'ì•„ë‹ˆì˜¤'}")
        print(f"ê°•ì œ ì¬í•™ìŠµ: {'ì˜ˆ' if force_retrain else 'ì•„ë‹ˆì˜¤'}")
        
        system = main_wiki_complaint_system(use_wiki_data=use_wiki, force_retrain=force_retrain)
    
    # ê°œë³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    if system and system.complaint_model:
        print("\n=== ê°œë³„ ë‹¨ì–´ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸ ===")
        test_words = ['ë°°ì†¡', 'í’ˆì§ˆ', 'ì„œë¹„ìŠ¤', 'ë¶ˆë§Œ', 'ê°œì„ ']
        for word in test_words:
            if word in system.complaint_model.wv.key_to_index:
                similar = system.complaint_model.wv.most_similar(word, topn=3)
                print(f"{word}: {[f'{w}({s:.2f})' for w, s in similar]}")
        
        # ì¶”ê°€ PyVis í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        try:
            print("\n=== PyVis ì¶”ê°€ í…ŒìŠ¤íŠ¸ ===")
            
            # ì»¤ìŠ¤í…€ ë‹¨ì–´ ëª©ë¡ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ìƒì„±
            custom_words = ['ë°°ì†¡', 'í’ˆì§ˆ', 'ì„œë¹„ìŠ¤', 'ë¶ˆë§Œ', 'ê°œì„ ', 'ê³ ê°', 'ë§Œì¡±']
            system.create_pyvis_network_graph(
                target_words=custom_words,
                similarity_threshold=0.1,
                output_file="custom_network.html"
            )
            
            # ë‹¤ë¥¸ ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ìƒì„±
            for focus in ['ì„œë¹„ìŠ¤', 'í’ˆì§ˆ']:
                if focus in system.complaint_model.wv.key_to_index:
                    system.create_interactive_similarity_network(
                        focus_word=focus,
                        top_n=10,
                        output_file=f"{focus}_network.html"
                    )
            
        except Exception as e:
            print(f"PyVis ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("\n=== ì‹¤í–‰ ì˜µì…˜ ì•ˆë‚´ ===")
    print("ê¸°ë³¸ ì‹¤í–‰: python korean_complaint_word2vec.py")
    print("ìœ„í‚¤í”¼ë””ì•„ ì—†ì´: python korean_complaint_word2vec.py --no-wiki")
    print("ê°•ì œ ì¬í•™ìŠµ: python korean_complaint_word2vec.py --force-retrain")
    print("ë¹ ë¥¸ í…ŒìŠ¤íŠ¸: python korean_complaint_word2vec.py --sample-only")
    
    print("\n=== PyVis ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ íŒŒì¼ ===")
    print("ğŸ“Š complaint_word_network.html - ì „ì²´ ë‹¨ì–´ ê´€ê³„ ë„¤íŠ¸ì›Œí¬")
    print("ğŸ¯ complaint_similarity_network.html - 'ë¶ˆë§Œ' ì¤‘ì‹¬ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬")
    print("âš¡ demo_network.html - ë¹ ë¥¸ ë°ëª¨ ë„¤íŠ¸ì›Œí¬")
    print("ğŸ”§ custom_network.html - ì»¤ìŠ¤í…€ ë‹¨ì–´ ë„¤íŠ¸ì›Œí¬")
    print("ğŸ“ˆ [ë‹¨ì–´]_network.html - íŠ¹ì • ë‹¨ì–´ ì¤‘ì‹¬ ë„¤íŠ¸ì›Œí¬")
    print("\nğŸ’¡ Tip: ì›¹ë¸Œë¼ìš°ì €ì—ì„œ HTML íŒŒì¼ì„ ì—´ì–´ ì¸í„°ë™í‹°ë¸Œ ê·¸ë˜í”„ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
